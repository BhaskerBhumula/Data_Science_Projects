{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285b82c3",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625680db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhumu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bhumu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23bb69",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de759ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\bhumu\\Downloads\\SPAM text message 20170820 - Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd52ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3c7002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7cf168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    object\n",
       "Message     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1cb6a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = dataset[\"Message\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5233e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb5ad5",
   "metadata": {},
   "source": [
    "#### Removing unwanted punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf887625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re_punt = \"[^A-Za-z\\s]\"\n",
    "mess = re.sub(re_punt, \"\",mess)\n",
    "mess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b89a9",
   "metadata": {},
   "source": [
    "#### Normalising the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad892bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess = mess.lower()\n",
    "mess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a41ef2",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b45671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c80c94c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "mess = nlp(mess)\n",
    "msg = []\n",
    "for token in mess:\n",
    "    msg.append(token.text)\n",
    "\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f79d5",
   "metadata": {},
   "source": [
    "#### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21cb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61b3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhumu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a317418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82192f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d073557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "518f743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [word for word in msg if word not in sw_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcea0439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "649115ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1046af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msgs = []\n",
    "# for word in msg:\n",
    "#     msgs.append(lemma.lemmatize(word, pos='v'))\n",
    "\n",
    "# msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07affce4",
   "metadata": {},
   "source": [
    "#### Entire text preprocessing using defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3c7971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(message):\n",
    "    re_punt = \"[^A-Za-z\\s]\"\n",
    "    message = re.sub(re_punt, \"\",message) #Removal of unwanted punctuations\n",
    "    message = message.lower() #Normalising the case\n",
    "    message = nlp(message) \n",
    "    \n",
    "    msg = [] # Tokenization\n",
    "    for token in message:\n",
    "        msg.append(token.text)\n",
    "        \n",
    "    sw_list = stopwords.words(\"english\") #Removal of stop words\n",
    "    msg = [word for word in msg if word not in sw_list]\n",
    "    \n",
    "    \n",
    "    msgs = ' ' #Lemmatization\n",
    "    for word in msg:\n",
    "        msgs += ' ' + lemma.lemmatize(word, pos='v')\n",
    "    \n",
    "    return msgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eccf1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Message1\"] = dataset[\"Message\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40b8fa39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Message1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry   wkly comp win fa cup final tkts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think go usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nd time try   contact u u   pound prize   cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>b go esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity   mood soany suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitch act like interest buy something el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  \\\n",
       "0         ham  Go until jurong point, crazy.. Available only ...   \n",
       "1         ham                      Ok lar... Joking wif u oni...   \n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3         ham  U dun say so early hor... U c already then say...   \n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...       ...                                                ...   \n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568      ham               Will ü b going to esplanade fr home?   \n",
       "5569      ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570      ham  The guy did some bitching but I acted like i'd...   \n",
       "5571      ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                               Message1  \n",
       "0       go jurong point crazy available bugis n grea...  \n",
       "1                                 ok lar joke wif u oni  \n",
       "2       free entry   wkly comp win fa cup final tkts...  \n",
       "3                   u dun say early hor u c already say  \n",
       "4                nah nt think go usf live around though  \n",
       "...                                                 ...  \n",
       "5567    nd time try   contact u u   pound prize   cl...  \n",
       "5568                             b go esplanade fr home  \n",
       "5569                      pity   mood soany suggestions  \n",
       "5570    guy bitch act like interest buy something el...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51babe73",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f240cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2f2e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "tfv = TfidfVectorizer()\n",
    "X = tfv.fit_transform(dataset['Message1']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d541f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5778c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns = tfv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33403262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zf</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zs</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 7476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aah  aaniye  aaooooright  aathilove  aathiwhere   ab  abbey  \\\n",
       "0     0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "1     0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "2     0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "3     0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "4     0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "...   ...  ...     ...          ...        ...         ...  ...    ...   \n",
       "5567  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "5568  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "5569  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "5570  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "5571  0.0  0.0     0.0          0.0        0.0         0.0  0.0    0.0   \n",
       "\n",
       "      abdomen  abeg  ...  zero   zf  zhong  zindgi  zoe  zogtorius  zoom  \\\n",
       "0         0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "1         0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "2         0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "3         0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4         0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "...       ...   ...  ...   ...  ...    ...     ...  ...        ...   ...   \n",
       "5567      0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "5568      0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "5569      0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "5570      0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "5571      0.0   0.0  ...   0.0  0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "\n",
       "      zouk   zs  zyada  \n",
       "0      0.0  0.0    0.0  \n",
       "1      0.0  0.0    0.0  \n",
       "2      0.0  0.0    0.0  \n",
       "3      0.0  0.0    0.0  \n",
       "4      0.0  0.0    0.0  \n",
       "...    ...  ...    ...  \n",
       "5567   0.0  0.0    0.0  \n",
       "5568   0.0  0.0    0.0  \n",
       "5569   0.0  0.0    0.0  \n",
       "5570   0.0  0.0    0.0  \n",
       "5571   0.0  0.0    0.0  \n",
       "\n",
       "[5572 rows x 7476 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb49ef",
   "metadata": {},
   "source": [
    "#### Encoding of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a6aed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a817aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7990fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data = ohe.fit_transform(np.array(dataset['Category']).reshape(len(dataset['Category']), 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccde5a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06a70d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers = [(\"ohe\", ohe, ['Category'])], remainder = \"passthrough\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a5f4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = ct.fit_transform(dataset)\n",
    "# new_df1 = pd.DataFrame(new_df, columns = ct.get_feature_names_out())\n",
    "# new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "430de996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ohe__Category_ham</th>\n",
       "      <th>ohe__Category_spam</th>\n",
       "      <th>remainder__Message</th>\n",
       "      <th>remainder__Message1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry   wkly comp win fa cup final tkts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think go usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nd time try   contact u u   pound prize   cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>b go esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity   mood soany suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitch act like interest buy something el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ohe__Category_ham ohe__Category_spam  \\\n",
       "0                  1.0                0.0   \n",
       "1                  1.0                0.0   \n",
       "2                  0.0                1.0   \n",
       "3                  1.0                0.0   \n",
       "4                  1.0                0.0   \n",
       "...                ...                ...   \n",
       "5567               0.0                1.0   \n",
       "5568               1.0                0.0   \n",
       "5569               1.0                0.0   \n",
       "5570               1.0                0.0   \n",
       "5571               1.0                0.0   \n",
       "\n",
       "                                     remainder__Message  \\\n",
       "0     Go until jurong point, crazy.. Available only ...   \n",
       "1                         Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     U dun say so early hor... U c already then say...   \n",
       "4     Nah I don't think he goes to usf, he lives aro...   \n",
       "...                                                 ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...   \n",
       "5568               Will ü b going to esplanade fr home?   \n",
       "5569  Pity, * was in mood for that. So...any other s...   \n",
       "5570  The guy did some bitching but I acted like i'd...   \n",
       "5571                         Rofl. Its true to its name   \n",
       "\n",
       "                                    remainder__Message1  \n",
       "0       go jurong point crazy available bugis n grea...  \n",
       "1                                 ok lar joke wif u oni  \n",
       "2       free entry   wkly comp win fa cup final tkts...  \n",
       "3                   u dun say early hor u c already say  \n",
       "4                nah nt think go usf live around though  \n",
       "...                                                 ...  \n",
       "5567    nd time try   contact u u   pound prize   cl...  \n",
       "5568                             b go esplanade fr home  \n",
       "5569                      pity   mood soany suggestions  \n",
       "5570    guy bitch act like interest buy something el...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1 = pd.DataFrame(new_df, columns = ct.get_feature_names_out())\n",
    "new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "055c82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df1[\"ohe__Category_ham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c07f750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       1.0\n",
       "2       0.0\n",
       "3       1.0\n",
       "4       1.0\n",
       "       ... \n",
       "5567    0.0\n",
       "5568    1.0\n",
       "5569    1.0\n",
       "5570    1.0\n",
       "5571    1.0\n",
       "Name: ohe__Category_ham, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1e88d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "792d16ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "5567    0\n",
       "5568    1\n",
       "5569    1\n",
       "5570    1\n",
       "5571    1\n",
       "Name: ohe__Category_ham, Length: 5572, dtype: int32"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e309c2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddc12e",
   "metadata": {},
   "source": [
    "#### Splitting the data into train-set and test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "988811de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.array(y), test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1342679",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96ca9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b49c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe451f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00dc48",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fda5ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d3429a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de63cf4",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e49a0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8484304932735426"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca7ded",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bf67b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.45      0.82      0.58       143\n",
      "         ham       0.97      0.85      0.91       972\n",
      "\n",
      "    accuracy                           0.85      1115\n",
      "   macro avg       0.71      0.84      0.74      1115\n",
      "weighted avg       0.90      0.85      0.87      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cls_names = ['spam', 'ham']\n",
    "print(classification_report(y_test, y_pred, target_names = cls_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ff725",
   "metadata": {},
   "source": [
    "#  Balanced data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea4775",
   "metadata": {},
   "source": [
    "#### Preparation of balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7bc4142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_indices = y[y == 0].index.tolist()\n",
    "ham_indices = y[y == 1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6d545688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating into two datasets, Spam or ham\n",
    "df_spam = X.iloc[spam_indices]\n",
    "df_ham = X.iloc[ham_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d4f2eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing total spam data into 80:20 ratio i.e., 597:150 because it has less count and ham data into 3860:965\n",
    "# Since we have more ham samples let's create 5 combined datasets (597 + 772) where ham set is divided into 5-sets and spam is present in \n",
    "# every set. \n",
    "\n",
    "#'''''''''''''' test_set ''''''''''''''#\n",
    "test_data = pd.concat([df_spam[:150], df_ham[:965]])\n",
    "\n",
    "\n",
    "#'''''''''''''' train sets preparation ''''''''''''''''''#\n",
    "train_data1 = pd.concat([df_spam[150:], df_ham[965:965+772]])\n",
    "\n",
    "train_data2 = pd.concat([df_spam[150:], df_ham[965+772:965+772*2]])\n",
    "\n",
    "train_data3 = pd.concat([df_spam[150:], df_ham[965+772*2:965+772*3]])\n",
    "\n",
    "train_data4 = pd.concat([df_spam[150:], df_ham[965+772*3:965+772*4]])\n",
    "\n",
    "train_data5 = pd.concat([df_spam[150:], df_ham[965+772*4:965+772*5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f3a92f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(597, 772)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spam[150:]), len(df_ham[965:965+772])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6bac5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_labels = np.zeros(len(df_spam[150:]), dtype = int)\n",
    "ham_labels = np.ones(len(df_ham[965:965+772]), dtype = int)\n",
    "\n",
    "train_labels = np.hstack((spam_labels, ham_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0df7d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test = np.zeros(150, dtype = int)\n",
    "ham_test = np.ones(965, dtype = int)\n",
    "\n",
    "test_labels = np.hstack((spam_test, ham_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615df50",
   "metadata": {},
   "source": [
    "#### Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "81aa47f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb1 = GaussianNB()\n",
    "nb1.fit(train_data1, train_labels)\n",
    "\n",
    "nb2 = GaussianNB()\n",
    "nb2.fit(train_data2, train_labels)\n",
    "\n",
    "nb3 = GaussianNB()\n",
    "nb3.fit(train_data3, train_labels)\n",
    "\n",
    "nb4 = GaussianNB()\n",
    "nb4.fit(train_data4, train_labels)\n",
    "\n",
    "nb5 = GaussianNB()\n",
    "nb5.fit(train_data5, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b8215",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "32a4f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7847533632286996\n",
      "0.8062780269058296\n",
      "0.7820627802690583\n",
      "0.8026905829596412\n",
      "0.7811659192825112\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = nb1.predict(test_data)\n",
    "print(accuracy_score(test_labels, y_pred1))\n",
    "\n",
    "y_pred2 = nb2.predict(test_data)\n",
    "print(accuracy_score(test_labels, y_pred2))\n",
    "\n",
    "y_pred3 = nb3.predict(test_data)\n",
    "print(accuracy_score(test_labels, y_pred3))\n",
    "\n",
    "y_pred4 = nb4.predict(test_data)\n",
    "print(accuracy_score(test_labels, y_pred4))\n",
    "\n",
    "y_pred5 = nb5.predict(test_data)\n",
    "print(accuracy_score(test_labels, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f9ab8",
   "metadata": {},
   "source": [
    "#### Ensemble predictions through Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3ebdc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sum = y_pred1 + y_pred2 + y_pred3 + y_pred4 + y_pred5\n",
    "\n",
    "pred_maj = [0 if i<=2 else 1 for i in pred_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "95199e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017937219730942"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, pred_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "425af487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.40      0.94      0.56       150\n",
      "         ham       0.99      0.78      0.87       965\n",
      "\n",
      "    accuracy                           0.80      1115\n",
      "   macro avg       0.69      0.86      0.72      1115\n",
      "weighted avg       0.91      0.80      0.83      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble approach classification report\n",
    "cls_names = ['spam', 'ham']\n",
    "print(classification_report(test_labels, pred_maj, target_names = cls_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4c8568a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.45      0.82      0.58       143\n",
      "         ham       0.97      0.85      0.91       972\n",
      "\n",
      "    accuracy                           0.85      1115\n",
      "   macro avg       0.71      0.84      0.74      1115\n",
      "weighted avg       0.90      0.85      0.87      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classification report\n",
    "print(classification_report(y_test, y_pred, target_names = cls_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a99019",
   "metadata": {},
   "source": [
    "# <font size = 2.8> Observation: We could see significant improvement in recall of spam classes through balanced data classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
